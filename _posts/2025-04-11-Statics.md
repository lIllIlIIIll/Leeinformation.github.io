---
layout: single
title:  "[AI  부트캠프] 프로젝트 수행을 위한 이론 - Statics"
mathjax: true
categories: Bootcamp
tag: [패스트캠퍼스, 패스트캠퍼스AI부트캠프, 업스테이지패스트캠퍼스, UpstageAILab, 국비지원, 패스트캠퍼스업스테이지에이아이랩, 패스트캠퍼스업스테이지부트캠프]
toc: true
author_profile: false
---

<head>
  <style>
    table.dataframe {
      white-space: normal;
      width: 100%;
      height: 240px;
      display: block;
      overflow: auto;
      font-family: Arial, sans-serif;
      font-size: 0.9rem;
      line-height: 20px;
      text-align: center;
      border: 0px !important;
    }

    table.dataframe th {
      text-align: center;
      font-weight: bold;
      padding: 8px;
    }

    table.dataframe td {
      text-align: center;
      padding: 8px;
    }

    table.dataframe tr:hover {
      background: #b8d1f3; 
    }

    .output_prompt {
      overflow: auto;
      font-size: 0.9rem;
      line-height: 1.45;
      border-radius: 0.3rem;
      -webkit-overflow-scrolling: touch;
      padding: 0.8rem;
      margin-top: 0;
      margin-bottom: 15px;
      font: 1rem Consolas, "Liberation Mono", Menlo, Courier, monospace;
      color: $code-text-color;
      border: solid 1px $border-color;
      border-radius: 0.3rem;
      word-break: normal;
      white-space: pre;
    }

  .dataframe tbody tr th:only-of-type {
      vertical-align: middle;
  }

  .dataframe tbody tr th {
      vertical-align: top;
  }

  .dataframe thead th {
      text-align: center !important;
      padding: 8px;
  }

  .page__content p {
      margin: 0 0 0px !important;
  }

  .page__content p > strong {
    font-size: 0.8rem !important;
  }

  </style>
</head>


***



기초적인 파이썬 문법, 라이브러리 강의가 끝나고 리서처로 활동하시는 강사분의 AI에 필요한 수학 이론 강의가 있었다.



확률과 통계, 미분(그레디언트) 등 여러가지가 필요했고, 고등학생 및 대학생 시절 배우지 않았던 부분도 존재하여 한편으로는 쉽기도 하고 헷갈리는 부분도 존재하였다.



***


# 1. 경우의 수 및 확률


***



첫 강의는 경우의 수 및 확률로, 합의 법칙, 곱의 법칙, 순열, 조합 등 경우의 수에서 다룰 수 있는 기본적인 개념과 이를 연결하여 합사건, 곱사건, 배반사건, 여사건 등의 확률까지 배웠었다.



해당 단원의 경우 학생 시절 공부하였던 부분이었기에, 강의를 듣다보니 복기되는 기분이 들었다.



특히 **수학적 확률**과 **통계적 확률** 부분에 있어 교과서에서 한번 보고 지나갔을 이론이 등장하였고, 다음과 같이 정의된다.



수학적 확률 : 표본공간 S 속 모든 원소가 나올 가능성이 동일하고 N(S) = N, 사건 A에 대하여 n(A) = n 일 때 A의 확률은



> $P(A) = \frac{n}{N}$



통계적 확률 : 시행횟수 N번을 크게 하면 상대도수로 나타나는 $r \over n$이 일정한 확률 값 $P(A)로 근사하게되고 정의는



> $${r \over n} \approx P(A), \quad \lim_{n \to \infty} \frac{r}{n} = P(A)$$



정의는 어렵지만 생각보다 단순하다.



수학적 확률은 흔히 우리가 생각할 수 있는 확률로, 주사위를 1번 던졌을 때 3이 나올 확률은 $1 \over 6$, 이런식으로 계산에 의해 정의되는 확률들을 말한다.



통계적 확률은 실제로 직접 시행하여 그 결과로 나오는 값을 의미하는데, 이 값이 시행 횟수가 크면 클수록 수학적 확률인 $P(A)$에 근사하는 것을 의미한다.



예를 들어, 주사위를 30번 던졌을 때, 3이 나올 확률은 $({1 \over 6})^{30}$인데, 실제로 시행했을 때 무조건 저 확률이 되리라는 보장이 없다. 단, 시행 횟수를 100번, 1000번 늘리면 늘릴수록 저 확률에 가까워 진다는 의미이다.



이외에도, 확률의 여러가지 법칙들과 조건부 확률 등 익숙했던 부분을 상기시켜주는 강의가 진행되었다.



***


# 2. 변수, 데이터, 척도


***



해당 단원은 통계학에서 기본적으로 사용되는 용어들의 의미, 정의들을 설명하는 단원이었다.



크게 **변수**와 **척도**로 나뉘었다.



- 변수 : 변화하는 모든 수



    ☞ 인과관계에 의한 구분

    - 독립변수 : 다른 변수에 영향을 주는 변수

    - 종속변수 : 독립변수에 의해 변화되는 변수



    ☞ 속성에 따라 구분

    - 질적변수 : 분류를 위하여 용어로 정의되는 변수

        - 비서열 질적변수 : 서열이 정해질 수 없는 변수

        - 서열 질적변수 : 서열적으로 구분할 수 있는 변수 

    - 양적변수 : 양의 크기를 나타내기 위하여 수량으로 표시되는 변수

        - 연속변수 : 주어진 범위 내에서 어떤 값도 가질 수 있는 변수

        - 비연속변수 : 특정 수치만을 가질 수 있는 변수



위와 같이 구분되며, 이런 변수들은 데이터로 구성이 되고, 이를 구분 지어 나누는 것을 척도라 한다.



- 척도



    - 범주형 척도 : 데이터를 구분지어 나눌 수 있는 척도

        - 명목 척도 : 수나 순서와 관계없이 이름만 붙여지는 척도

        - 서열 척도 : 숫자나 연산과는 관련이 없으나, 순서를 구분할 수 있는 척도



    - 연속형 척도 : 연속하는 속성의 데이터를 연구나 조사의 목적에 맞게 구분한 척도

        - 등간 척도 : 측정한 자료들을 대상으로 합과 차가 가능한 척도

        - 비율 척도 : 등간 척도의 성질과 함께 '없다'의 개념인 0 값도 가지는 척도



사실 이런 변수와 척도는 우리가 실생활에서 무의식적으로 생각되고 있는 개념들이다.



단지, 이름을 붙여놓은 것으로 쉽게 이해할 수 있었다.



***


# 3. 모집단, 표본추출


***



해당 단원에서는 알고 있었던 용어들 이외에도 낯선 단어들이 굉장히 많이 나왔다.



모집단의 특성값인 모수, 모집단의 일부인 표본부터, 여러가지 표본추출 방법이 나왔고, 이중 가장 인상 깊었던 표본추출 방법인 **비례 층화 표본추출** 방법을 소개한다.



> **비례 층화 표본추출**이란 모집단을 여러 개의 다른 집단으로 구분(특성이 비슷한 것들끼리) 후, 각 집단의 구성을 고려하여 비례적으로 추출하는 방법이다.



해당 표본추출 방법은 이전에 머신러닝 모델을 훈련시킬 때, 사용하였던 다운샘플링 방식과 유사하게 느껴졌다.



데이터가 너무 많아 컴퓨터 리소스가 부족하여 데이터를 줄여야했을 때, 특정 특성을 기준으로 데이터를 구분, 구분된 집단에서의 데이터의 양을 고려하여 일부를 추출하여 추출된 데이터로 훈련하였었다.



> 여기서 연결하여 **다단계 층화 표본추출**이란 개념도 존재하였고, **비례 층화 표본추출**에서 상위-하위 표본 단위를 설정, 설정한 값에 따라 다시 추출하는 방법이다.



내가 알고있었던 개념, 쓰고 있었지만 몰랐던 개념, 아예 처음 들어본 개념들까지 되짚거나 새로 알게되는 용어들이 많았고, 이와 같이 정의하는 과정은 차후 어떤 프로젝트에 대해 설명할 때 알아두어야 할 개념들로 익숙해질 필요성을 느꼈다.



***


# 4. 기술 통계량 및 가설 검정


***



**Pandas, Matplotlib**과 같은 파이썬 데이터 분석 및 시각화 툴을 통해 데이터의 전체적인 개요를 파악할 수 있다면 이를 가공하기 위한 통계량들을 알아야한다.



평균, 최빈값, 중앙값 등 계속해서 사용했던 개념들과, 모분산과 표본분산(표본표준편차), 이상치 판단을 위한 IQR의 기본이되는 사분위수 등 많이 접해있는 개념들로 실제로 경진대회, 프로젝트등과 같이 Pandas 라이브러리 사용 시 굉장히 많이 사용되는 개념들이다.



위와 같은 통계량 이외에도 왜도와 첨도라는 자료의 분포를 나타내는 통계량도 존재하였다.



> 왜도




![image.png](https://github.com/lIllIlIIIll/Leeinformation.github.io/blob/master/_posts/image/Bootcamp/왜도.PNG?raw=true)




> 첨도




![image.png](https://github.com/lIllIlIIIll/Leeinformation.github.io/blob/master/_posts/image/Bootcamp/첨도.PNG?raw=true)




이외에도 여러 추정(점추정, 구간추정), 이를 통해 측정된 값이 특정 구간에 포함될 확률인 신뢰구간을 알려주셨고 전에 배웠던 개념이 어디에 사용되는지 알려주셨다.



사실 이 수학 강의들 중 가장 헷갈렸던 부분은 가설 검정이었다.



**귀무가설, 대립가설** 이 부분과 가설 검정의 오류가 굉장히 헷갈렸는데 먼저 각 개념들의 정의는



> 귀무가설 : 입증하고자 하는 가설, 일반적으로 믿어온 사실



> 대립가설 : 연구의 목적이되는 가설



예를 들어보자면,



- A 회사의 약은 효과가 없다. → 귀무가설



- A 회사의 약은 효과가 있다. → 대립가설



사실 개념으로 외우기보다 나는 ***~~이 없다 → 귀무가설, ~~이 있다 → 대립가설*** 로 외우는 것이 쉬웠다.



이를 바탕으로 연구 or 조사에서 범할 수 있는 오류는 다음과 같다.



- 1종 오류 : 귀무가설이 참이지만 귀무가설을 기각



- 2종 오류 : 대립가설이 참이지만 대립가설을 기각



이를 표로 나타내면 다음과 같다.



|             | 귀무가설 참       | 귀무가설 거짓     |

|-------------|-------------------|-------------------|

| **채택**    | 옳은 결정         | 2종 오류          |

| **기각**    | 1종 오류          | 옳은 결정         |



이 부분은 뭔가 말장난 같이 느껴져서 더 헷갈렸다. ~~가 없다가 참이면 어떻게되고 ~~가 있다가 거짓이면 어떻게되고 이런 식이라서 더 그랬던것 같다.



***


# 5. 벡터, 행렬


***



벡터와 행렬은 머신러닝 및 딥러닝에 있어 필수불가결한 개념들로, 정규화 및 인공신경망 구현 등 많은 곳에 쓰이는 개념들이다.



먼저 벡터 단원에서 가장 인상깊었던건 노름이었다.



먼저 노름의 정의는 벡터가 원점에서 얼마나 떨어져 있는지 의미하는 값이다.



**L1 노름** : 벡터에 포함된 요소의 절대값의 합



> $\|x\|_1 = \sum_{i=1}^{\infty} |x_i|$



**L2 노름** : 벡터에 포함된 요소의 제곱합의 제곱근



> $\|x\|_2 = \sqrt{ \sum_{i=1}^{\infty} |x_i|^2 }$



L1 정규화, L2 정규화 등 하이퍼파라미터 조정 시 사용했었는데, 정확히 어떤 의미를 가지고 어떻게 정의되는지를 자세히 몰랐기에 이번 기회에 알수 있어서 좋았다.







***







행렬은 학부생 시절 1학년때부터 4학년 졸업할때까지 계속해서 사용했던 개념이었다.



교양 수업에서부터, 전공까지 사용처가 너무 많았기에 까먹을래야 까먹을 수 없었고, 기본적인 연산들 및 파이썬의 **torch**를 이용하여 이미지를 대칭 이동시키는 등의 활용까지 해봤었던 연산들이었다.



사실 이런 개념들을 바탕으로 평균 벡터, 공분산 행렬을 배웠는데 여기가 굉장히 어려웠다.



먼저 평균 벡터는 쉬웠는데, 벡터가 주어졌을 때, 각 벡터의 요소들의 위치에 맞춰 더하고 벡터의 갯수로 나눠주면 되었기에 쉽게 계산할 수 있었다.



그러나 공분산 행렬의 경우 **PCA**에서 사용된다는 것을 알았지만 어떤식으로 계산되는지 아예 몰랐기에 집중했었고, 굉장히 어려웠다.



먼저 공분산이란 두 확률 변수 간 선형 관계를 나타내는 지표로, 두 변수 값이 함께 변화하는 정도를 측정한 것이다.



이때, 여러 변수들의 공분산을 행렬 형태로 표현한 것이 공분산 행렬이고 수식은 다음과 같다.



- $S^2 = \frac{1}{n - 1} \sum_{i=1}^{n} (x_i - \mu)^2$



이를 구하는 방법은 다음과 같다.



1. 주어진 벡터로 데이터 행렬 작성



2. 주어진 벡터의 평균 벡터 구하기



3. 데이터 행렬에서 평균 벡터 빼기(중심화)



4. 평균 벡터를 뺀 행렬과 그 전치행렬을 행렬곱 수행



***


# 6. 확률 변수 및 확률 함수, 분포


***



확률 변수 단원은 용어들이 어려워 보일 뿐 굉장히 쉽다.



고등학생 때 틀리면 안되었던 문제였고, 그저 주어진 식에 맞춰 성질을 이용할 줄 아는지를 물어보는 개념들이었다.



다음은 여러 분포들(균등분포, 정규분포 등)에 대해서 배웠다.



이 중 정규분포가 가장 중요하기도 하고 이를 표준화하는 표준정규분포를 서로 다른 표준정규분포를 비교하기 위해 많이 사용한다.



정규분포란 어떤 사건이 일어난 빈도를 계산하여 그래프로 나타내면 중심을 기준으로 좌우가 대칭되는 다음과 같은 분포이다.




![image.png](https://github.com/lIllIlIIIll/Leeinformation.github.io/blob/master/_posts/image/Bootcamp/정규분포.PNG?raw=true)





이 분포에서 평균을 0, 표준편차를 1로 표준화하면 표준정규분포가 된다.



정규분포 또한, 데이터를 분석 및 전처리할 때 많이 사용하기도 하고, 중요한 개념이라 이미 알고 있었기에 복습의 느낌으로 넘어갔다.



***


# 7. 선형 회귀 및 이진 분류


***



머신러닝 학습에 있어 기본이 되는 개념이다.



**회귀**는 모델을 학습시켜 새로운 데이터에 대한 결과를 예측하는 과정이다.



**분류**는 모델을 학습시켜 새로운 데이터가 어느 범주에 속하는지 예측하는 과정이다.



흔히 머신러닝에 사용되는건 비선형 회귀와 다중 클래스 분류이지만 기초를 다지는 과정으로 선형 회귀와 이진 분류를 알려주셨다.



선형 회귀의 경우 특성이 한 개이기 때문에 이를 데이터의 분포를 가장 잘 나타내는 기울기인 가중치와 $y$절편인 편향으로 1차 방정식을 찾는 과정으로 볼 수 있다.



이 과정에서 1차 방정식의 그래프와 실제 주어진 데이터들의 차이가 존재하는데(오차) 이를 계산하기 위한 **손실함수**가 존재하고 오차 계산 시, $(데이터의 실제 값 - y)^2$ 으로 표현된다.



***



모델이 잘 예측하도록, 즉 손실함수가 최소가 되는 지점을 찾기 위해 가장 많이 쓰이는 방법인 **경사 하강법**은 그래프를 그리지 않고 최솟값을 찾는 방법과 유사하다.



선택된 특정 지점(가중치 $W$)에서 미분하여 기울기를 구하고 그 기울기가 작아지는 방향으로 나아가다보면 기울기가 작아지지 않는 지점을 찾을 수 있게 되고 이는 손실함수의 최솟값(지역적일 수도 있다.)에 도달할 수 있게 된다.



이 경사 하강법은 학부생 시절 레포트로 직접 조사 및 실험하여 제출한 적도 있기에 너무 잘 알고 있는 개념이었다.



***



다음은 분류 알고리즘으로 로지스틱 회귀, 시그모이드, 이진 크로스 엔트로피에 대해 설명해 주셨다.



마찬가지로 복습하는 느낌으로 가져갔고 각각의 알고리즘은 간단하게 다음과 같이 정의할 수 있다.



- 로지스틱 회귀 : 훈련 데이터의 특성, 분포를 나타내는 최적의 직선을 찾고 그 선을 기준으로 위, 아래(왼쪽, 오른쪽)으로 분류



- 시그모이드 : 훈련 데이터의 변수가 회귀에 입력으로 들어가고 이 값이 이진 분류에 입력으로 들어가서 계산, 계산된 값은 0과 1사이의 실수로 표현되며, 특정 값을 기준으로 참과 거짓 → 이진 분류로 구현



- 이진 크로스 엔트로피 : 시그모이드 함수로 계산된 값과 오차를 토대로 다음과 같이 함수식을 표현하여 가중치와 편향을 구해 손실함수가 최소가 되는 지점을 찾는다.



> $E(W, b) = -\sum_{i=1}^{n} \left[ t_i \log(y_i) + (1 - t_i) \log(1 - y_i) \right]$



***


# 8. 마치며


***



익숙했던 개념, 생소한 개념 여러가지가 섞여 쉽기도 하고 어렵기도 한 강의였다.



특히 배웠던 개념을 한 단계 더 깊게 연결시켜 나가는 과정은 무엇이든 간에 어렵고 익숙하지 않다.



하지만 앞으로 있을 머신러닝 및 딥러닝 알고리즘에 사용되는 중요한 개념이기도 하고 몰랐던 것을 하나하나 알아가고 기록하는 것은 모두 자산이 될 것이다.



만약 차후 부트캠프 진행이던, 취업 후 필요하던 이러한 수학 이론들은 강사분께서 해주셨던 말처럼 용어를 기억했다가 필요하면 참고할 수 있는 좋은 강의였다.

